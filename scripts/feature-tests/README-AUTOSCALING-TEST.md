# ì˜¤í† ìŠ¤ì¼€ì¼ë§ ê¸°ëŠ¥ ê²€ì¦ ê°€ì´ë“œ

AI Storage Orchestratorì˜ ì˜¤í† ìŠ¤ì¼€ì¼ë§ ê¸°ëŠ¥ì„ ê²€ì¦í•˜ê¸° ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ ëª¨ìŒì…ë‹ˆë‹¤.

## ğŸ“‹ ìŠ¤í¬ë¦½íŠ¸ ê°œìš”

### 1. test-autoscaling.sh (ë©”ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸)
ì˜¤í† ìŠ¤ì¼€ì¼ë§ ê¸°ëŠ¥ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ìë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

**ìˆ˜í–‰ ì‘ì—…:**
1. AI Storage Orchestrator ì‹¤í–‰ ìƒíƒœ í™•ì¸
2. DCGM Exporter ì‹¤í–‰ ìƒíƒœ í™•ì¸
3. GPU í…ŒìŠ¤íŠ¸ ì›Œí¬ë¡œë“œ ë°°í¬
4. GPU ë…¸ë“œ ë¼ë²¨ í™•ì¸
5. Orchestratorë¡œ í¬íŠ¸í¬ì›Œë“œ ì„¤ì • (8080 í¬íŠ¸)
6. ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±
7. 3ë¶„ ë™ì•ˆ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§ (30ì´ˆë§ˆë‹¤ ì²´í¬)
8. ê²°ê³¼ ì¶œë ¥

**ì‹¤í–‰ ë°©ë²•:**
```bash
cd /root/workspace/ai-storage-orchestrator
./scripts/test-autoscaling.sh
```

**ì˜ˆìƒ ì¶œë ¥:**
```
==========================================
AI Storage Orchestrator Autoscaling Test
==========================================

[STEP] Checking if AI Storage Orchestrator is running...
[SUCCESS] AI Storage Orchestrator is running

[STEP] Checking if DCGM Exporter is running...
[SUCCESS] DCGM Exporter is running

[STEP] Deploying GPU test workload...
[SUCCESS] GPU test workload deployed

[STEP] Creating autoscaler for GPU workload...
[SUCCESS] Autoscaler created with ID: as-12345678

[STEP] Monitoring autoscaler for 3 minutes...
----------------------------------------
Check #1 (14:30:15)
----------------------------------------
  Replicas: 1 (desired: 1)
  CPU: 5% (target: 70%)
  Memory: 12% (target: 80%)
  GPU: 0% (target: 60%)
  Scale events: UP=0, DOWN=0
```

### 2. simulate-gpu-load.sh (ë¶€í•˜ ì‹œë®¬ë ˆì´ì…˜)
ë‹¤ì–‘í•œ GPU ë¶€í•˜ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.

**ì‹œë‚˜ë¦¬ì˜¤:**
- Scenario 1: ë‚®ì€ ë¶€í•˜ (1 replica)
- Scenario 2: ì¤‘ê°„ ë¶€í•˜ (2 replicas)
- Scenario 3: ë†’ì€ ë¶€í•˜ (3 replicas)

**ì‹¤í–‰ ë°©ë²•:**
```bash
cd /root/workspace/ai-storage-orchestrator
./scripts/simulate-gpu-load.sh
```

**ì‚¬ìš© ì‹œê¸°:**
- test-autoscaling.sh ì‹¤í–‰ ì¤‘ ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰
- ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ì˜ ìŠ¤ì¼€ì¼ ì—…/ë‹¤ìš´ ë™ì‘ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì„ ë•Œ

### 3. cleanup-autoscaling-test.sh (ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸)
í…ŒìŠ¤íŠ¸ í›„ ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

**ìˆ˜í–‰ ì‘ì—…:**
1. ëª¨ë“  ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ ì‚­ì œ
2. í¬íŠ¸í¬ì›Œë“œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
3. GPU í…ŒìŠ¤íŠ¸ ì›Œí¬ë¡œë“œ ì‚­ì œ (ì„ íƒì‚¬í•­)
4. DCGM Exporter ì‚­ì œ (ì„ íƒì‚¬í•­)

**ì‹¤í–‰ ë°©ë²•:**
```bash
cd /root/workspace/ai-storage-orchestrator
./scripts/cleanup-autoscaling-test.sh
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (3ë¶„ ì†Œìš”)

```bash
cd /root/workspace/ai-storage-orchestrator

# 1. ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
./scripts/test-autoscaling.sh

# 2. ê²°ê³¼ í™•ì¸ í›„ ì •ë¦¬
./scripts/cleanup-autoscaling-test.sh
```

### ë¶€í•˜ ì‹œë®¬ë ˆì´ì…˜ í¬í•¨ í…ŒìŠ¤íŠ¸

**í„°ë¯¸ë„ 1 (ëª¨ë‹ˆí„°ë§):**
```bash
cd /root/workspace/ai-storage-orchestrator
./scripts/test-autoscaling.sh
```

**í„°ë¯¸ë„ 2 (ë¶€í•˜ ìƒì„±):**
```bash
cd /root/workspace/ai-storage-orchestrator
# ë©”ì¸ í…ŒìŠ¤íŠ¸ê°€ ëª¨ë‹ˆí„°ë§ ë‹¨ê³„ì— ë“¤ì–´ê°„ í›„ ì‹¤í–‰
./scripts/simulate-gpu-load.sh
```

## ğŸ“Š ë©”íŠ¸ë¦­ ì„¤ëª…

### ì¶œë ¥ ë©”íŠ¸ë¦­ í•´ì„

```
Replicas: 1 (desired: 2)
```
- **current replicas**: í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ Pod ìˆ˜
- **desired replicas**: ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì›í•˜ëŠ” Pod ìˆ˜
- ë‘ ê°’ì´ ë‹¤ë¥´ë©´ ìŠ¤ì¼€ì¼ë§ì´ ì§„í–‰ ì¤‘

```
CPU: 5% (target: 70%)
Memory: 12% (target: 80%)
GPU: 0% (target: 60%)
```
- **current**: í˜„ì¬ í‰ê·  ì‚¬ìš©ë¥ 
- **target**: ëª©í‘œ ì‚¬ìš©ë¥ 
- í˜„ì¬ê°’ì´ targetë³´ë‹¤ ë†’ìœ¼ë©´ scale up ê°€ëŠ¥ì„±
- í˜„ì¬ê°’ì´ targetë³´ë‹¤ ë‚®ìœ¼ë©´ scale down ê°€ëŠ¥ì„±

```
Scale events: UP=2, DOWN=1
```
- **UP**: ìŠ¤ì¼€ì¼ ì—… ë°œìƒ íšŸìˆ˜
- **DOWN**: ìŠ¤ì¼€ì¼ ë‹¤ìš´ ë°œìƒ íšŸìˆ˜

## ğŸ” ì˜ˆìƒ ë™ì‘

### ì •ìƒ ì‹œë‚˜ë¦¬ì˜¤

1. **ì´ˆê¸° ìƒíƒœ (1 replica)**
   - GPU ì‚¬ìš©ë¥ : 0-10% (nvidia-smië§Œ ì‹¤í–‰)
   - ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬: í˜„ì¬ ìƒíƒœ ìœ ì§€

2. **ë¶€í•˜ ì¦ê°€ ì‹œ**
   - GPU ì‚¬ìš©ë¥ ì´ 60% ì´ˆê³¼
   - 30ì´ˆ í›„: ìŠ¤ì¼€ì¼ ì—… ê¶Œì¥
   - Stabilization window í›„: ì‹¤ì œ ìŠ¤ì¼€ì¼ ì—… ìˆ˜í–‰

3. **ë¶€í•˜ ê°ì†Œ ì‹œ**
   - GPU ì‚¬ìš©ë¥ ì´ 60% ë¯¸ë§Œ
   - 5ë¶„ ë™ì•ˆ ë‚®ì€ ì‚¬ìš©ë¥  ìœ ì§€ (ê¸°ë³¸ stabilization window)
   - ì´í›„: ìŠ¤ì¼€ì¼ ë‹¤ìš´ ìˆ˜í–‰

### GPU ë©”íŠ¸ë¦­ ì†ŒìŠ¤

**DCGM Exporter ì‚¬ìš© ê°€ëŠ¥ ì‹œ:**
- ì‹¤ì œ GPU ì‚¬ìš©ë¥  ìˆ˜ì§‘
- Podë³„ GPU ë©”íŠ¸ë¦­ ì œê³µ
- ì •í™•í•œ ìŠ¤ì¼€ì¼ë§ ê²°ì •

**DCGM Exporter ì‚¬ìš© ë¶ˆê°€ ì‹œ:**
- Fallback: ì‹œë®¬ë ˆì´ì…˜ ê°’ (60-90%)
- ë¡œê·¸ì— "Failed to get real metrics" ë©”ì‹œì§€ ì¶œë ¥
- ì˜¤í† ìŠ¤ì¼€ì¼ë§ì€ ê³„ì† ë™ì‘

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ: "AI Storage Orchestrator is not running"

**ì›ì¸:** Orchestratorê°€ ë°°í¬ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì¤‘ë‹¨ë¨

**í•´ê²°:**
```bash
cd /root/workspace/ai-storage-orchestrator
./scripts/build.sh
./scripts/deploy.sh
kubectl wait --for=condition=available --timeout=120s deployment/ai-storage-orchestrator -n kube-system
```

### ë¬¸ì œ: "DCGM Exporter is not running"

**ì›ì¸:** DCGM Exporterê°€ ë°°í¬ë˜ì§€ ì•ŠìŒ

**í•´ê²°:**
```bash
kubectl apply -f /root/workspace/ai-storage-orchestrator/deployments/dcgm-exporter.yaml
kubectl wait --for=condition=ready --timeout=120s pod -l app=dcgm-exporter -n gpu-monitoring
```

### ë¬¸ì œ: "No GPU nodes found"

**ì›ì¸:** GPU ë…¸ë“œì— í•„ìš”í•œ ë¼ë²¨ì´ ì—†ìŒ

**í•´ê²°:**
```bash
# GPUê°€ ìˆëŠ” ë…¸ë“œ ì´ë¦„ í™•ì¸
kubectl get nodes

# ë¼ë²¨ ì¶”ê°€
kubectl label nodes <gpu-node-name> nvidia.com/gpu=present
```

### ë¬¸ì œ: "Failed to connect to orchestrator"

**ì›ì¸:** í¬íŠ¸í¬ì›Œë“œ ì‹¤íŒ¨

**í•´ê²°:**
```bash
# ê¸°ì¡´ í¬íŠ¸í¬ì›Œë“œ ì¢…ë£Œ
pkill -f "kubectl port-forward.*ai-storage-orchestrator"

# ìˆ˜ë™ìœ¼ë¡œ í¬íŠ¸í¬ì›Œë“œ ì„¤ì •
kubectl port-forward -n kube-system svc/ai-storage-orchestrator 8080:8080

# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸
curl http://localhost:8080/health
```

### ë¬¸ì œ: "GPU utilization always 0%"

**ì˜ˆìƒ ë™ì‘:** í…ŒìŠ¤íŠ¸ ì›Œí¬ë¡œë“œëŠ” nvidia-smië§Œ ì‹¤í–‰í•˜ë¯€ë¡œ GPU ì‚¬ìš©ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤.

**ì‹¤ì œ GPU ë¶€í•˜ ìƒì„±:**
```bash
# GPU ì—°ì‚° ë¶€í•˜ë¥¼ ìƒì„±í•˜ëŠ” Pod ë°°í¬
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: gpu-burn
spec:
  containers:
  - name: gpu-burn
    image: nvidia/cuda:11.8.0-base-ubuntu22.04
    command: ["sh", "-c", "apt-get update && apt-get install -y git build-essential && git clone https://github.com/wilicc/gpu-burn && cd gpu-burn && make && ./gpu_burn 300"]
    resources:
      limits:
        nvidia.com/gpu: 1
EOF
```

## ğŸ“ ìˆ˜ë™ í…ŒìŠ¤íŠ¸

ìŠ¤í¬ë¦½íŠ¸ ì—†ì´ ìˆ˜ë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:

### 1. í¬íŠ¸í¬ì›Œë“œ ì„¤ì •
```bash
kubectl port-forward -n kube-system svc/ai-storage-orchestrator 8080:8080
```

### 2. ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±
```bash
curl -X POST http://localhost:8080/api/v1/autoscalers \
  -H "Content-Type: application/json" \
  -d '{
    "workload_name": "gpu-test-workload",
    "workload_namespace": "default",
    "workload_type": "deployment",
    "min_replicas": 1,
    "max_replicas": 5,
    "target_cpu_percent": 70,
    "target_memory_percent": 80,
    "target_gpu_percent": 60,
    "scale_check_interval": 30
  }'
```

### 3. ìƒíƒœ í™•ì¸
```bash
# ëª¨ë“  ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ ì¡°íšŒ
curl http://localhost:8080/api/v1/autoscalers

# íŠ¹ì • ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„¸ ì¡°íšŒ (IDëŠ” ìƒì„± ì‹œ ë°˜í™˜ëœ ê°’)
curl http://localhost:8080/api/v1/autoscalers/<autoscaling-id>
```

### 4. ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ ì‚­ì œ
```bash
curl -X DELETE http://localhost:8080/api/v1/autoscalers/<autoscaling-id>
```

## ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ

í…ŒìŠ¤íŠ¸ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì§€í‘œ:

1. **ë°˜ì‘ ì‹œê°„**: ë¶€í•˜ ë³€í™” í›„ ìŠ¤ì¼€ì¼ë§ê¹Œì§€ì˜ ì‹œê°„
2. **ì•ˆì •ì„±**: Flapping ì—†ì´ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€
3. **ì •í™•ì„±**: GPU ë©”íŠ¸ë¦­ì´ ì‹¤ì œ ì‚¬ìš©ë¥ ì„ ë°˜ì˜í•˜ëŠ”ì§€
4. **Stabilization Window**: ìŠ¤ì¼€ì¼ ë‹¤ìš´ì´ ë„ˆë¬´ ê¸‰ê²©í•˜ì§€ ì•Šì€ì§€

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [Autoscaling API Guide](../docs/autoscaling_api_guide.md)
- [DCGM Setup Guide](../docs/dcgm_setup_guide.md)
- [CLAUDE.md](../CLAUDE.md)

## ğŸ’¡ íŒ

1. **ë¡œê·¸ ëª¨ë‹ˆí„°ë§**: í…ŒìŠ¤íŠ¸ ì¤‘ ë¡œê·¸ë¥¼ ë³´ë©´ ë” ìì„¸í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
   ```bash
   kubectl logs -n kube-system -l app=ai-storage-orchestrator -f
   ```

2. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: watch ëª…ë ¹ì–´ë¡œ ì‹¤ì‹œê°„ ìƒíƒœ í™•ì¸
   ```bash
   watch -n 5 'curl -s http://localhost:8080/api/v1/autoscalers/<id>'
   ```

3. **GPU ë©”íŠ¸ë¦­ ì§ì ‘ í™•ì¸**: DCGM Exporter ë©”íŠ¸ë¦­ ì§ì ‘ ì¡°íšŒ
   ```bash
   kubectl run test-curl --image=curlimages/curl:latest --rm -i --restart=Never -- \
     curl -s http://dcgm-exporter.gpu-monitoring.svc.cluster.local:9400/metrics | grep DCGM_FI_DEV_GPU_UTIL
   ```
